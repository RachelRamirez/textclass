unnest_tokens(word, description) %>%
anti_join(sw) %>%
count( word, sort = TRUE)
ITdata <- readRDS("IT_total.RDS")
setwd("C:/Users/Charter/Desktop/williamcsevier/github/textclass/topic_models")
ITdata <- readRDS("IT_total.RDS")
data <- ITdata
#how many of each PSC
sw <- add_row(stop_words, word = c("igf","ot", "ct"), lexicon = c("SMART", "SMART", "SMART"))
plt <- data %>%
unnest_tokens(word, description) %>%
anti_join(sw) %>%
count( word, sort = TRUE)
ggplot(plt, aes(word, n)) +
geom_bar(stat = "identity")
library(dplyr)
library(ggplot2)
library(tidytext)
library(tm)
data <- ITdata
#how many of each PSC
sw <- add_row(stop_words, word = c("igf","ot", "ct"), lexicon = c("SMART", "SMART", "SMART"))
plt <- data %>%
unnest_tokens(word, description) %>%
anti_join(sw) %>%
count( word, sort = TRUE)
ggplot(plt, aes(word, n)) +
geom_bar(stat = "identity")
plt <- data %>%
unnest_tokens(word, description) %>%
anti_join(sw) %>%
count( word, sort = TRUE) %>%
top_n(10,n)
ggplot(plt, aes(word, n)) +
geom_bar(stat = "identity")
ggplot(plt, aes(word, sort(n))) +
geom_bar(stat = "identity")
ggplot(plt, aes(word, sort(n))) +
geom_bar(stat = "identity") +
coord_flip()
ggplot(plt, aes(word, order(n))) +
geom_bar(stat = "identity") +
coord_flip()
ggplot(plt, aes(word, desc(n))) +
geom_bar(stat = "identity") +
coord_flip()
ggplot(plt, aes(word, -n)) +
geom_bar(stat = "identity") +
coord_flip()
ggplot(plt, aes(word, n)) +
geom_bar(stat = "identity") +
coord_flip()
ggplot(plt, aes(reorder(word, -n), n)) +
geom_bar(stat = "identity") +
coord_flip()
ggplot(plt, aes(reorder(word, n), n)) +
geom_bar(stat = "identity") +
coord_flip()
ggplot(plt, aes(reorder(word, n), n)) +
geom_bar(stat = "identity") +
xlab("Term") +
ylab("Count") +
coord_flip()
shiny::runApp()
runApp()
library(textclass)
setwd("C:/Users/Charter/Desktop/williamcsevier/github/textclass")
library(textclass)
runApp('topic_models')
library(textclass)
runApp('topic_models')
runApp('topic_models')
runApp('topic_models')
library(textclass)
runApp('topic_models')
sw <- add_row(stop_words, word = c("igf","ot", "ct"), lexicon = c("SMART", "SMART", "SMART"))
plt <- data %>%
unnest_tokens(word, description, token = "ngrams", n = n) %>%
anti_join(sw) %>%
count( word, sort = TRUE) %>%
top_n(10,n)
library(textclass)
runApp('topic_models')
runApp('topic_models')
plt <- data %>%
unnest_tokens(word, description, token = "ngrams", n = 2) %>%
anti_join(sw) %>%
count( word, sort = TRUE) %>%
top_n(10,n)
plt
plt <- data %>%
unnest_tokens(word, description, token = "ngrams", n = 3) %>%
separate(word, c("word1", "word2", "word3"), sep = " ") %>%
filter(!word1 %in% sw$word) %>%
filter(!word2 %in% sw$word) %>%
filter(!word3 %in% sw$word) %>%
unite("word", c(word1, word2,word3) sep ="") %>%
count( word, sort = TRUE) %>%
top_n(10,n)
plt <- data %>%
unnest_tokens(word, description, token = "ngrams", n = 3) %>%
separate(word, c("word1", "word2", "word3"), sep = " ") %>%
filter(!word1 %in% sw$word) %>%
filter(!word2 %in% sw$word) %>%
filter(!word3 %in% sw$word) %>%
unite("word", c(word1, word2,word3), sep ="") %>%
count( word, sort = TRUE) %>%
top_n(10,n)
plt
library(textclass)
runApp('topic_models')
plt
plt <- data %>%
unnest_tokens(word, description, token = "ngrams", n = 4) %>%
separate(word, c("word1", "word2", "word3", "word4"), sep = " ") %>%
filter(!word1 %in% sw$word) %>%
filter(!word2 %in% sw$word) %>%
filter(!word3 %in% sw$word) %>%
filter(!word4 %in% sw$word) %>%
unite("word", c(word1, word2, word3, word4), sep =" ") %>%
count( word, sort = TRUE) %>%
top_n(10,n)
plt
library(textclass)
runApp('topic_models')
runApp('topic_models')
ggplot(plt, aes(reorder(word, n), n)) +
geom_bar(stat = "identity") +
xlab("Term") +
ylab("Count") +
coord_flip()
library(textclass)
runApp('topic_models')
data %>%
unnest_tokens(word, description, token = "ngrams", n = n) %>%
separate(word, c("word1", "word2", "word3"), sep = " ") %>%
filter(!word1 %in% sw$word) %>%
filter(!word2 %in% sw$word) %>%
filter(!word3 %in% sw$word) %>%
unite("word", c(word1, word2, word3), sep =" ") %>%
count( word, sort = TRUE) %>%
top_n(10,n)
data %>%
unnest_tokens(word, description, token = "ngrams", n = 3) %>%
separate(word, c("word1", "word2", "word3"), sep = " ") %>%
filter(!word1 %in% sw$word) %>%
filter(!word2 %in% sw$word) %>%
filter(!word3 %in% sw$word) %>%
unite("word", c(word1, word2, word3), sep =" ") %>%
count( word, sort = TRUE) %>%
top_n(10,n)
data %>%
unnest_tokens(word, description, token = "ngrams", n = n) %>%
separate(word, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% sw$word) %>%
filter(!word2 %in% sw$word) %>%
unite("word", word1, word2, sep =" ") %>%
count( word, sort = TRUE) %>%
top_n(10,n)
data %>%
unnest_tokens(word, description, token = "ngrams", n = 2) %>%
separate(word, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% sw$word) %>%
filter(!word2 %in% sw$word) %>%
unite("word", word1, word2, sep =" ") %>%
count( word, sort = TRUE) %>%
top_n(10,n)
term_frequency <- function(data,n){
sw <- add_row(stop_words, word = c("igf","ot", "ct"), lexicon = c("SMART", "SMART", "SMART"))
if(n == 2){
plt <- data %>%
unnest_tokens(word, description, token = "ngrams", n = n) %>%
separate(word, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% sw$word) %>%
filter(!word2 %in% sw$word) %>%
unite("word", word1, word2, sep =" ") %>%
count( word, sort = TRUE) %>%
top_n(10,n)
}
else if(n==3){
plt <- data %>%
unnest_tokens(word, description, token = "ngrams", n = n) %>%
separate(word, c("word1", "word2", "word3"), sep = " ") %>%
filter(!word1 %in% sw$word) %>%
filter(!word2 %in% sw$word) %>%
filter(!word3 %in% sw$word) %>%
unite("word", c(word1, word2, word3), sep =" ") %>%
count( word, sort = TRUE) %>%
top_n(10,n)
}
else if(n==4){
plt <- data %>%
unnest_tokens(word, description, token = "ngrams", n = n) %>%
separate(word, c("word1", "word2", "word3", "word4"), sep = " ") %>%
filter(!word1 %in% sw$word) %>%
filter(!word2 %in% sw$word) %>%
filter(!word3 %in% sw$word) %>%
filter(!word4 %in% sw$word) %>%
unite("word", c(word1, word2, word3, word4), sep =" ") %>%
count( word, sort = TRUE) %>%
top_n(10,n)
}
else if(n>4){
print("too many n-grams")
}
else{
plt <- data %>%
unnest_tokens(word, description) %>%
anti_join(sw) %>%
count( word, sort = TRUE) %>%
top_n(10,n)}
plt <- ggplot(plt, aes(reorder(word, n), n)) +
geom_bar(stat = "identity") +
xlab("Term") +
ylab("Count") +
coord_flip()
return(plt)
}
term_frequency(data,1)
term_frequency(data,2)
term_frequency(data,3)
library(textclass)
runApp('topic_models')
runApp('topic_models')
install.packages(ldatuning)
install.packages("ldatuning")
library(textclass)
library(textclass)
library(textclass)
library(roxygen2)
library(textclass)
library(textclass)
devtools::check(textclass)
devtools::check("textclass")
setwd("C:/Users/Charter/Desktop/williamcsevier/github")
devtools::check("textclass")
library(textclass)
?term_frequency()
library(textclass)
setwd("C:/Users/Charter/Desktop/williamcsevier/github/textclass")
data <- readRDS("ITdata.RDS")
data <- readRDS("topic_models\ITdata.RDS")
data <- readRDS("topic_models/ITdata.RDS")
data <- readRDS("./topic_models/ITdata.RDS")
data <- readRDS("./topic_models/IT_total.RDS")
AFICAdata <- data
devtools::use_data(AFICAdata)
devtools::document()
devtools::document()
devtools::load_all
devtools::load_all()
?AFICAdata
str(AFICAdata)
devtools::document()
devtools::load_all()
devtools::document()
devtools::load_all()
?AFICAdata
devtools::document()
devtools::load_all()
?AFICAdata
devtools::use_testthat
devtools::use_testthat()
library(textclass)
?term_frequency
.GlobalEnv
View(term_frequency)
unloadNamespace("term_frequency")
rm(term_frequency)
library(textclass)
devtools::use_vignette("textclass")
runApp('topic_models')
install.packages("shinyjs")
shiny::runApp('topic_models')
runApp('topic_models')
runApp('topic_models')
runApp('topic_models')
runApp('topic_models')
runApp('topic_models')
runApp('topic_models')
library(textclass)
library(textclass)
runApp()
library(textclass)
runApp()
library(textclass)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
vignette(textclass)
vignette()
vignette("textclass")
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
?stringr
vignette(package = "tidyverse")
vignette(package = "tidytext")
??Stringr
install.packages("ggraph")
runApp()
install.packages("tweenr")
runApp()
runApp()
install.packages("igraph")
runApp()
runApp()
runApp()
?igraph
install.packages("igraph")
runApp()
install.packages("igraph", force = TRUE)
install.packages("igraph", dependencies = TRUE)
runApp()
.libPaths()
library("igraph")
shiny::runApp('topic_models')
bigrams <- ITdata %>%
unnest_tokens(bigram, description, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% sw$word,
!word2 %in% sw$word) %>%
count(word1, word2, sort = TRUE)
bigrams <- bigrams %>%
filter(n > 40,
!str_detect(word1, "\\d"),
!str_detect(word2, "\\d"))
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
plt <- bigrams %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void()
plt
bigrams <- ITdata %>%
unnest_tokens(bigram, description, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% sw$word,
!word2 %in% sw$word) %>%
count(word1, word2, sort = TRUE)
bigrams <- bigrams %>%
filter(n > 1000,
!str_detect(word1, "\\d"),
!str_detect(word2, "\\d"))
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
plt <- bigrams %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void()
plt
bigrams <- ITdata %>%
unnest_tokens(bigram, description, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% sw$word,
!word2 %in% sw$word) %>%
count(word1, word2, sort = TRUE)
bigrams <- bigrams %>%
filter(n > 500,
!str_detect(word1, "\\d"),
!str_detect(word2, "\\d"))
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
plt <- bigrams %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void()
plt
runApp('topic_models')
runApp('topic_models')
library(textclass)
runApp('topic_models')
devtools::install_github("williamcsevier/textclass")
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
values <- optimal_topics(AFICAdata)
#tidyDTM(data, sparsity)
dtm <- tidyDTM(AFICAdata, 0.98)
dtm
values <- optimal_topics(dtm)
#tidyDTM(data, sparsity)
dtm <- tidyDTM(AFICAdata, 0.70)
dtm
#tidyDTM(data, sparsity)
dtm <- tidyDTM(AFICAdata, 0.90)
dtm
#tidyDTM(data, sparsity)
dtm <- tidyDTM(AFICAdata, 0.97)
dtm
#tidyDTM(data, sparsity)
dtm <- tidyDTM(AFICAdata, 0.98)
dtm
values <- optimal_topics(dtm)
library(textclass)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
#tidyDTM(data, sparsity)
dtm <- tidyDTM(AFICAdata, 0.98)
library(textclass)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
#tidyDTM(data, sparsity)
dtm <- tidyDTM(AFICAdata, 0.98)
dtm
values <- optimal_topics(dtm)
.libPaths()
.libPaths(.libPaths()[1])
.libPaths()
.libPaths()
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
term_frequency(AFICAdata, n=3)
library(textclass)
term_frequency(AFICAdata, n=3)
devtools::install_github("williamcsevier/textclass", dependencies = TRUE)
term_frequency(AFICAdata, n=3)
?add_row
library("dplyr", lib.loc="~/R/win-library/3.4")
?tibble
?add_row
term_frequency(AFICAdata, n=3)
?stop_words
?tm
term_frequency(AFICAdata, n=3)
?tidy_text
library(textclass)
term_frequency(AFICAdata, n=3)
library(textclass)
term_frequency(AFICAdata, n=3)
install.packages("tidytext")
term_frequency(AFICAdata, n=3)
library(tidytext)
term_frequency(AFICAdata, n=3)
install.packages("tidyverse")
library(tidyverse)
term_frequency(AFICAdata, n=3)
#tidyDTM(data, sparsity)
AFICAdata <- AFICAdata %>% sample(10000)
#tidyDTM(data, sparsity)
AFICAdata <- AFICAdata %>% sample(1000)
#tidyDTM(data, sparsity)
AFICAdata
dtm <- tidyDTM(AFICAdata, 0.98)
install.packages("tm")
library(tm)
#tidyDTM(data, sparsity)
dtm <- tidyDTM(AFICAdata, 0.98)
dtm
lda <- LDA(dtm, k = 4)
install.packages("topic_models")
install.packages("topicmodels")
library(topicmodels)
lda <- LDA(dtm, k = 4)
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm <- dtm[rowTotals> 0, ]
lda <- LDA(dtm, k = 4)
plot_topics(lda)
values <- optimal_topics(dtm)
install.packages("ldatuning")
library(ldatuning)
values <- optimal_topics(dtm)
install.packages("modeltools")
library(modeltools)
values <- optimal_topics(dtm)
install.packages("ldatuning", dependencies = TRUE)
install.packages("ldatuning", dependencies = TRUE)
unloadNamespace(ldatuning)
unloadNamespace("ldatuning")
install.packages("ldatuning", dependencies = TRUE)
values <- optimal_topics(dtm)
library(ldatuning)
values <- optimal_topics(dtm)
?slam
install.packages("slam")
library(slam)
values <- optimal_topics(dtm)
?slam
library(slam)
values <- optimal_topics(dtm)
